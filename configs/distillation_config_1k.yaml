# FirstSight: Knowledge Distillation Configuration - 1K SAMPLES
# Teacher: Qwen2-VL-7B-Instruct â†’ Student: Qwen2-VL-2B-Instruct
# Mid-scale evaluation (1000 samples)

teacher:
  model_name: "Qwen/Qwen2-VL-7B-Instruct"
  load_in_8bit: true  # Use INT8 quantization to save memory
  device_map: "auto"
  torch_dtype: "bfloat16"

student:
  model_name: "Qwen/Qwen2-VL-2B-Instruct"
  torch_dtype: "bfloat16"  # BF16 for training stability
  device_map: "auto"
  gradient_checkpointing: true

training:
  # Training hyperparameters - 1K SCALE
  epochs: 10
  batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-4
  warmup_steps: 100
  
  # Distillation parameters
  temperature: 3.0
  alpha_logit: 0.9
  beta_feature: 0.1
  
  # Optimization
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Mixed precision
  use_amp: true

data:
  # Data configuration - 1K SAMPLES
  train_samples: 1000
  val_samples: 200
  num_workers: 4
  train_data_path: "data/egoqa_train_5k.json"  # Will sample 1000 from this
  val_data_path: "data/egoqa_val_1k.json"

hpc:
  gpus: 1
  mem_gb: 80
  time_hours: 12
  
logging:
  log_interval: 10
  save_checkpoint_epochs: 1
  
output:
  checkpoint_dir: "experiments/distillation_1k"  # DIFFERENT directory!
  best_model_name: "best_student_model"
  final_model_name: "final_student_model"

