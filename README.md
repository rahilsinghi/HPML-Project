# FirstSight: Efficient Egocentric Question Answering

**Team:** Rahil Singhi (rs9174@nyu.edu) & Sunidhi Tandel (sdt9243@nyu.edu)  
**Course:** ECE-GY 9143 - High Performance Machine Learning  
**Institution:** NYU Tandon School of Engineering

---

## ğŸ“‹ Project Overview

FirstSight builds an end-to-end pipeline for **efficient egocentric question answering** using Vision-Language Models (VLMs). The project focuses on:

1. **Knowledge Distillation**: Compress large egocentric VLMs (EgoGPT-7b) into efficient student models (Qwen2-VL-2B)
2. **Memory Optimization**: INT8 quantization and parameter-efficient training (LoRA)
3. **Edge Deployment**: Deploy compressed models on resource-constrained devices (AR glasses, mobile GPUs)

### Target Metrics

| Metric | Target | Status |
|--------|--------|--------|
| Training Throughput | â‰¥2Ã— improvement | ğŸ”„ In Progress |
| VRAM Reduction | â‰¥30% savings | âœ… Achieved (INT8) |
| Accuracy Preservation | â‰¤1% degradation | ğŸ”„ Testing |

---

## ğŸ—ï¸ Repository Structure

```
firstsight/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ distillation/             # Knowledge distillation (Phase 3)
â”‚   â”‚   â”œâ”€â”€ distill_vlm.py        # Main training loop
â”‚   â”‚   â”œâ”€â”€ models.py             # Model loading utilities
â”‚   â”‚   â”œâ”€â”€ data.py               # Data preparation
â”‚   â”‚   â””â”€â”€ evaluate.py           # Evaluation & comparison
â”‚   â”œâ”€â”€ baseline/                 # Baseline experiments (Phase 1)
â”‚   â”‚   â”œâ”€â”€ baseline_eval.py      # Profiling Qwen2-VL-2B
â”‚   â”‚   â””â”€â”€ quantization_eval.py  # FP16 vs INT8 comparison
â”‚   â””â”€â”€ utils/                    # Shared utilities
â”‚       â””â”€â”€ generate_report.py    # Report generation
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â””â”€â”€ distillation_config.yaml  # Distillation hyperparameters
â”œâ”€â”€ scripts/                      # Executable scripts
â”‚   â”œâ”€â”€ hpc_setup.sh              # HPC environment setup
â”‚   â”œâ”€â”€ upload_to_hpc.sh          # Upload files to HPC
â”‚   â””â”€â”€ slurm/                    # SLURM job configurations
â”‚       â”œâ”€â”€ run_baseline.slurm
â”‚       â”œâ”€â”€ run_quantization.slurm
â”‚       â””â”€â”€ run_distillation.slurm
â”œâ”€â”€ experiments/                  # Experiment outputs
â”‚   â”œâ”€â”€ baseline/
â”‚   â”œâ”€â”€ quantization/
â”‚   â””â”€â”€ distillation/
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ midterm/                  # Archived mid-term reports
â”‚   â””â”€â”€ README_distillation.md    # Distillation guide
â”œâ”€â”€ notebooks/                    # Jupyter notebooks (optional)
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸš€ Quick Start

### Prerequisites

- Access to NYU HPC Greene cluster
- Python 3.8+
- CUDA 11.3+ compatible GPU (A100 recommended)

### 1. Upload to HPC

```bash
# On your local machine
cd /path/to/firstsight
bash scripts/upload_to_hpc.sh --all
```

### 2. Setup Environment on HPC

```bash
# SSH to HPC
ssh rs9174@greene.hpc.nyu.edu

# Navigate to project
cd /scratch/rs9174/firstsight

# Setup environment (first time only)
bash scripts/hpc_setup.sh --mode venv

# This installs:
# - PyTorch 2.1.0 + CUDA 11.8
# - Transformers 4.36.0
# - PEFT, bitsandbytes, accelerate
# - Profiling tools
```

### 3. Run Experiments

#### Option A: Baseline Profiling

```bash
# Measure baseline performance
sbatch scripts/slurm/run_baseline.slurm

# Compare FP16 vs INT8 quantization
sbatch scripts/slurm/run_quantization.slurm
```

#### Option B: Knowledge Distillation (Main Focus)

```bash
# Run distillation: EgoGPT-7b â†’ Qwen2-VL-2B
sbatch scripts/slurm/run_distillation.slurm

# Monitor progress
squeue -u rs9174
tail -f distillation_*.out
```

### 4. Download Results

```bash
# On your local machine
scp -r rs9174@greene.hpc.nyu.edu:/scratch/rs9174/firstsight/experiments ./
```

---

## ğŸ“Š Experiments

### Phase 1: Baseline & Quantization (Completed âœ…)

**Baseline Profiling** (`src/baseline/baseline_eval.py`):
- Model: Qwen2-VL-2B-Instruct
- Metrics: Load time, VRAM, latency, throughput
- Results: `experiments/baseline/`

**Quantization** (`src/baseline/quantization_eval.py`):
- Comparison: FP16 vs INT8
- Expected savings: ~30-40% VRAM reduction
- Results: `experiments/quantization/`

### Phase 3: Knowledge Distillation (Current ğŸ”„)

**Teacher Model**: [EgoGPT-7b-EgoIT](https://huggingface.co/EgoGPT/EgoGPT-7b-EgoIT) (9B params, egocentric-specialized)  
**Student Model**: Qwen2-VL-2B-Instruct (2B params, general VLM)

**Distillation Strategy**:
- Logit distillation (KL divergence)
- Temperature scaling (T=3.0)
- Mixed precision training (BF16)
- Gradient checkpointing

**Expected Results**:
- Compression: 9B â†’ 2B (~78% reduction)
- Speedup: ~3-4Ã— faster inference
- Performance: 85-90% of teacher accuracy
- VRAM: ~2.8GB (INT8 deployed)

See [`docs/README_distillation.md`](docs/README_distillation.md) for detailed guide.

---

## ğŸ”§ Configuration

Edit [`configs/distillation_config.yaml`](configs/distillation_config.yaml) to customize:

```yaml
teacher:
  model_name: "EgoGPT/EgoGPT-7b-EgoIT"
  load_in_8bit: true  # Use INT8 to save memory

student:
  model_name: "Qwen/Qwen2-VL-2B-Instruct"
  torch_dtype: "bfloat16"
  gradient_checkpointing: true

training:
  epochs: 3
  batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-4
  temperature: 3.0
  alpha_logit: 0.7
```

---

## ğŸ“ˆ Results Summary

### Baseline Results (Qwen2-VL-2B)

| Metric | Value |
|--------|-------|
| Model VRAM | 4.2 GB (FP16) |
| Avg Latency | 0.52s per query |
| Throughput | 1.9 queries/sec |
| Peak VRAM | 5.1 GB |

### Quantization Results (FP16 vs INT8)

| Metric | FP16 | INT8 | Improvement |
|--------|------|------|-------------|
| VRAM | 4.2 GB | 2.8 GB | **33% â†“** |
| Latency | 0.52s | 0.48s | 8% faster |
| Throughput | 1.9 q/s | 2.1 q/s | 10% â†‘ |

### Distillation Results (Teacher vs Student)

*(Results will be updated after distillation experiment)*

| Metric | Teacher (9B) | Student (2B) | Change |
|--------|--------------|--------------|--------|
| Parameters | 9B | 2B | 78% â†“ |
| VRAM | ~4.5 GB | ~2.8 GB | TBD |
| Latency | TBD | TBD | TBD |
| Accuracy | Baseline | TBD | TBD |

---

## ğŸ› ï¸ Development

### Running Locally (Testing)

```bash
# Install dependencies
pip install -r requirements.txt

# Test data loading
python -m src.distillation.data

# Test model loading
python -m src.distillation.models

# Run small-scale distillation
python -m src.distillation.distill_vlm configs/distillation_config.yaml
```

### Adding New Features

1. Create new module in `src/`
2. Add configuration to `configs/`
3. Create SLURM job in `scripts/slurm/`
4. Update `scripts/upload_to_hpc.sh`
5. Document in `docs/`

---

## ğŸ“š Key References

1. **EgoGPT**: Egocentric Vision-Language Model (Teacher)
   - [HuggingFace Model](https://huggingface.co/EgoGPT/EgoGPT-7b-EgoIT)
   
2. **Knowledge Distillation**:
   - Hinton et al., "Distilling the Knowledge in a Neural Network" (2015)
   - Sanh et al., "DistilBERT" (2019)

3. **Modality-Balanced Quantization**:
   - Liu et al., "Modality Balanced Quantization for Large Vision-Language Models" (2024) - [arXiv:2412.19509](https://arxiv.org/abs/2412.19509)

4. **Parameter-Efficient Fine-Tuning**:
   - Hu et al., "LoRA: Low-Rank Adaptation" (2021) - [arXiv:2106.09685](https://arxiv.org/abs/2106.09685)

5. **Quantization**:
   - Dettmers et al., "8-bit Optimizers via Block-wise Quantization" (2022) - [arXiv:2110.02861](https://arxiv.org/abs/2110.02861)

---

## ğŸ‘¥ Team Contributions

### Sunidhi Tandel
- Fine-tuning infrastructure
- CUDA-level optimizations
- Performance profiling
- Baseline evaluation scripts

### Rahil Singhi
- Knowledge distillation implementation
- HPC setup and SLURM configuration
- Model compression and quantization
- Documentation and project organization

### Joint Work
- Project design and methodology
- Experimental evaluation
- Literature review
- Report generation

---

## ğŸ“ License

This project is developed for academic purposes as part of NYU's High Performance Machine Learning course.

---

## ğŸ¤ Acknowledgments

- **NYU HPC Team** for compute resources
- **Hugging Face** for model hosting and transformers library
- **EgoGPT Team** for the egocentric VLM
- **Qwen Team** for Qwen2-VL models

---

## ğŸ“ Contact

- Rahil Singhi: rs9174@nyu.edu
- Sunidhi Tandel: sdt9243@nyu.edu

For questions, issues, or collaboration opportunities, please reach out via email.

---

**Last Updated**: December 2024

